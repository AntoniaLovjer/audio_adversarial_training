{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_training.ipynb\t     evaluation.ipynb  models\t    saved\r\n",
      "audio_training_output.ipynb  logs\t       __pycache__  utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pDrv-WQ2QVn"
   },
   "source": [
    "## Extract data from zip file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4546,
     "status": "ok",
     "timestamp": 1572873608402,
     "user": {
      "displayName": "Antonia Lovjer",
      "photoUrl": "",
      "userId": "16395681258946800452"
     },
     "user_tz": 300
    },
    "id": "lAIamznNy8fo",
    "outputId": "7d5d965f-9b54-4384-a991-f6cc7236a6fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "p7zip-full is already the newest version (16.02+dfsg-3+deb9u1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# import package to unzip .7zip file\n",
    "!sudo apt-get install -y p7zip-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 113960,
     "status": "ok",
     "timestamp": 1572873725080,
     "user": {
      "displayName": "Antonia Lovjer",
      "photoUrl": "",
      "userId": "16395681258946800452"
     },
     "user_tz": 300
    },
    "id": "05cAj2gwyQ2k",
    "outputId": "4658aa78-5d58-4b21-c4fc-24f41f2b4031",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# locally unzip the audio files in train.7z\n",
    "# !cd ../Data/ && p7zip -d train.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../Data/train/audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4KBXX6szo28"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14LKOMt0y3rX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, lfilter\n",
    "import scipy.ndimage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import log_textfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'normalTrain1'\n",
    "logfile = './logs/' + MODELNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jpi0p5JP0n24"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3511,
     "status": "ok",
     "timestamp": 1572873952952,
     "user": {
      "displayName": "Antonia Lovjer",
      "photoUrl": "",
      "userId": "16395681258946800452"
     },
     "user_tz": 300
    },
    "id": "61eZwZCm1iho",
    "outputId": "3a4508b2-719f-479b-a03e-2c4da2be0ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21105 train and 2577 val samples\n"
     ]
    }
   ],
   "source": [
    "DATADIR = '../Data/' # unzipped train and test data\n",
    "OUTDIR = './model-k' # just a random name\n",
    "# Data Loading\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\" Return 2 lists of tuples:\n",
    "    [(class_id, user_id, path), ...] for train\n",
    "    [(class_id, user_id, path), ...] for validation\n",
    "    \"\"\"\n",
    "    # Just a simple regexp for paths with three groups:\n",
    "    # prefix, label, user_id\n",
    "    pattern = re.compile(\"(.+\\/)?(\\w+)\\/([^_]+)_.+wav\")\n",
    "    all_files = glob(os.path.join(data_dir, 'train/audio/*/*wav'))\n",
    "\n",
    "    with open(os.path.join(data_dir, 'train/validation_list.txt'), 'r') as fin:\n",
    "        validation_files = fin.readlines()\n",
    "    valset = set()\n",
    "    for entry in validation_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            valset.add(r.group(3))\n",
    "\n",
    "    possible = set(LABELS)\n",
    "    train, val = [], []\n",
    "    for entry in all_files:\n",
    "        bl_true = True\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            label, uid = r.group(2), r.group(3)\n",
    "            if label == '_background_noise_':\n",
    "                label = 'silence'\n",
    "                bl_true = False\n",
    "            if label not in possible:\n",
    "                label = 'unknown'\n",
    "                bl_true = False\n",
    "            \n",
    "            if bl_true:\n",
    "                label_id = name2id[label]\n",
    "\n",
    "                sample = (label_id, uid, entry)\n",
    "                if uid in valset:\n",
    "                    val.append(sample)\n",
    "                else:\n",
    "                    train.append(sample)\n",
    "\n",
    "    print('There are {} train and {} val samples'.format(len(train), len(val)))\n",
    "    return train, val\n",
    "\n",
    "trainset, valset = load_data(DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnUYIUcKfwiI"
   },
   "source": [
    "## Make Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0w66pVcNq698"
   },
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "fft_size = 2048 # window size for the FFT\n",
    "step_size = fft_size/16 # distance to slide along the window (in time)\n",
    "spec_thresh = 4 # threshold for spectrograms (lower filters out more noise)\n",
    "lowcut = 500 # Hz # Low cut for our butter bandpass filter\n",
    "highcut = 15000 # Hz # High cut for our butter bandpass filter\n",
    "# For mels\n",
    "n_mel_freq_components = 64 # number of mel frequency channels\n",
    "shorten_factor = 10 # how much should we compress the x-axis (time)\n",
    "start_freq = 300 # Hz # What frequency to start sampling our melS from \n",
    "end_freq = 8000 # Hz # What frequency to stop sampling our melS from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4sMN6drfptl"
   },
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def overlap(X, window_size, window_step):\n",
    "    \"\"\"\n",
    "    Create an overlapped version of X\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape=(n_samples,)\n",
    "        Input signal to window and overlap\n",
    "    window_size : int\n",
    "        Size of windows to take\n",
    "    window_step : int\n",
    "        Step size between windows\n",
    "    Returns\n",
    "    -------\n",
    "    X_strided : shape=(n_windows, window_size)\n",
    "        2D array of overlapped X\n",
    "    \"\"\"\n",
    "    window_size, window_step = map(int, (window_size, window_step))\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(\"Window size must be even!\")\n",
    "    # Make sure there are an even number of windows before stridetricks\n",
    "    append = np.zeros((window_size - len(X) % window_size))\n",
    "    X = np.hstack((X, append))\n",
    "\n",
    "    ws = window_size\n",
    "    ss = window_step\n",
    "    a = X\n",
    "\n",
    "    valid = len(a) - ws\n",
    "    nw = (valid) // ss\n",
    "    out = np.ndarray((nw,ws),dtype = a.dtype)\n",
    "\n",
    "    for i in range(nw):\n",
    "        # \"slide\" the window along the samples\n",
    "        start = i * ss\n",
    "        stop = start + ws\n",
    "        out[i] = a[start : stop]\n",
    "\n",
    "    return out\n",
    "\n",
    "def stft(X, fftsize=128, step=65, mean_normalize=True, real=False,\n",
    "         compute_onesided=True):\n",
    "    \"\"\"\n",
    "    Compute STFT for 1D real valued input X\n",
    "    \"\"\"\n",
    "    X = np.copy(X)\n",
    "    if real:\n",
    "        local_fft = np.fft.rfft\n",
    "        cut = -1\n",
    "    else:\n",
    "        local_fft = np.fft.fft\n",
    "        cut = None\n",
    "    if compute_onesided:\n",
    "        cut = fftsize // 2\n",
    "    if mean_normalize:\n",
    "        X -= X.mean()\n",
    "\n",
    "    X = overlap(X, fftsize, step)\n",
    "    \n",
    "    size = fftsize\n",
    "    win = 0.54 - .46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "    X = X * win[None]\n",
    "    X = local_fft(X)[:, :cut]\n",
    "    return X\n",
    "\n",
    "def pretty_spectrogram(d,log = True, thresh= 5, fft_size = 512, step_size = 64):\n",
    "    \"\"\"\n",
    "    creates a spectrogram\n",
    "    log: take the log of the spectrgram\n",
    "    thresh: threshold minimum power for log spectrogram\n",
    "    \"\"\"\n",
    "    specgram = np.abs(stft(d, fftsize=fft_size, step=step_size, real=False,\n",
    "        compute_onesided=True))\n",
    "  \n",
    "    if log == True:\n",
    "        specgram /= specgram.max() # volume normalize to max 1\n",
    "        specgram = np.log10(specgram) # take log\n",
    "        specgram[specgram < -thresh] = -thresh # set anything less than the threshold as the threshold\n",
    "    else:\n",
    "        specgram[specgram < thresh] = thresh # set anything less than the threshold as the threshold\n",
    "    \n",
    "    return specgram\n",
    "\n",
    "# Also mostly modified or taken from https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "def invert_pretty_spectrogram(X_s, log = True, fft_size = 512, step_size = 512/4, n_iter = 10):\n",
    "    \n",
    "    if log == True:\n",
    "        X_s = np.power(10, X_s)\n",
    "\n",
    "    X_s = np.concatenate([X_s, X_s[:, ::-1]], axis=1)\n",
    "    X_t = iterate_invert_spectrogram(X_s, fft_size, step_size, n_iter=n_iter)\n",
    "    return X_t\n",
    "\n",
    "def iterate_invert_spectrogram(X_s, fftsize, step, n_iter=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    reg = np.max(X_s) / 1E8\n",
    "    X_best = copy.deepcopy(X_s)\n",
    "    for i in range(n_iter):\n",
    "        if verbose:\n",
    "            print(\"Runnning iter %i\" % i)\n",
    "        if i == 0:\n",
    "            X_t = invert_spectrogram(X_best, step, calculate_offset=True,\n",
    "                                     set_zero_phase=True)\n",
    "        else:\n",
    "            # Calculate offset was False in the MATLAB version\n",
    "            # but in mine it massively improves the result\n",
    "            # Possible bug in my impl?\n",
    "            X_t = invert_spectrogram(X_best, step, calculate_offset=True,\n",
    "                                     set_zero_phase=False)\n",
    "        est = stft(X_t, fftsize=fftsize, step=step, compute_onesided=False)\n",
    "        phase = est / np.maximum(reg, np.abs(est))\n",
    "        X_best = X_s * phase[:len(X_s)]\n",
    "    X_t = invert_spectrogram(X_best, step, calculate_offset=True,\n",
    "                             set_zero_phase=False)\n",
    "    return np.real(X_t)\n",
    "\n",
    "def invert_spectrogram(X_s, step, calculate_offset=True, set_zero_phase=True):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    step = int(step)\n",
    "    size = int(X_s.shape[1] // 2)\n",
    "    wave = np.zeros((X_s.shape[0] * step + size))\n",
    "    # Getting overflow warnings with 32 bit...\n",
    "    wave = wave.astype('float64')\n",
    "    total_windowing_sum = np.zeros((X_s.shape[0] * step + size))\n",
    "    win = 0.54 - .46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "\n",
    "    est_start = int(size // 2) - 1\n",
    "    est_end = est_start + size\n",
    "    for i in range(X_s.shape[0]):\n",
    "        wave_start = int(step * i)\n",
    "        wave_end = wave_start + size\n",
    "        if set_zero_phase:\n",
    "            spectral_slice = X_s[i].real + 0j\n",
    "        else:\n",
    "            # already complex\n",
    "            spectral_slice = X_s[i]\n",
    "\n",
    "        # Don't need fftshift due to different impl.\n",
    "        wave_est = np.real(np.fft.ifft(spectral_slice))[::-1]\n",
    "        if calculate_offset and i > 0:\n",
    "            offset_size = size - step\n",
    "            if offset_size <= 0:\n",
    "                print(\"WARNING: Large step size >50\\% detected! \"\n",
    "                      \"This code works best with high overlap - try \"\n",
    "                      \"with 75% or greater\")\n",
    "                offset_size = step\n",
    "            offset = xcorr_offset(wave[wave_start:wave_start + offset_size],\n",
    "                                  wave_est[est_start:est_start + offset_size])\n",
    "        else:\n",
    "            offset = 0\n",
    "        wave[wave_start:wave_end] += win * wave_est[\n",
    "            est_start - offset:est_end - offset]\n",
    "        total_windowing_sum[wave_start:wave_end] += win\n",
    "    wave = np.real(wave) / (total_windowing_sum + 1E-6)\n",
    "    return wave\n",
    "\n",
    "def xcorr_offset(x1, x2):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    x1 = x1 - x1.mean()\n",
    "    x2 = x2 - x2.mean()\n",
    "    frame_size = len(x2)\n",
    "    half = frame_size // 2\n",
    "    corrs = np.convolve(x1.astype('float32'), x2[::-1].astype('float32'))\n",
    "    corrs[:half] = -1E30\n",
    "    corrs[-half:] = -1E30\n",
    "    offset = corrs.argmax() - len(x1)\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ms9WQvOBf0bL"
   },
   "source": [
    "## Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQCQFWgk9O1P"
   },
   "outputs": [],
   "source": [
    "# from torchvision.transforms import Compose\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomDatasetSimple_new():\n",
    "    \"\"\"Simple dataset class for dataloader\"\"\"\n",
    "    def __init__(self, X, y, mean, std):\n",
    "        \"\"\"Initialize the CustomDataset\"\"\"\n",
    "\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total length of the dataset\"\"\"\n",
    "        dataset_size = len(self.X)\n",
    "        return dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the batch given the indices\"\"\"\n",
    "        # print('debug1')\n",
    "        rate, data = wavfile.read(self.X[idx])\n",
    "        spec = pretty_spectrogram(data.astype('float64'), fft_size = fft_size, step_size = step_size, log = True, thresh = spec_thresh)\n",
    "        height = spec.shape[0]\n",
    "        if height!=112:\n",
    "            spec = pad_dimesions(spec)\n",
    "        # print('debug2')\n",
    "        X = np.copy(spec)\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        X.unsqueeze_(-1)\n",
    "        X = X.transpose(2, 0)\n",
    "        X = X.transpose(2, 1)\n",
    "        # print('debug3')\n",
    "        X_batch = (X-self.mean)/self.std\n",
    "        y_batch = self.y[idx]\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJuRYaKOueaZ"
   },
   "source": [
    "## Define Attack Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afxS7j8ougwc"
   },
   "outputs": [],
   "source": [
    "def fgsm(model, X, y, epsilon=0.3):\n",
    "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    delta = delta.cuda()\n",
    "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "    loss.backward()\n",
    "    return epsilon * delta.grad.detach().sign()\n",
    "\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=20, randomize=False):\n",
    "    \"\"\" Construct PGD adversarial examples on the examples X\"\"\"\n",
    "    if randomize:\n",
    "        delta = torch.rand_like(X, requires_grad=True)\n",
    "        delta.data = delta.data * 2 * epsilon - epsilon\n",
    "    else:\n",
    "        delta = torch.zeros_like(X, requires_grad=True)\n",
    "        \n",
    "    for t in range(num_iter):\n",
    "        loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsiliYP4qiev"
   },
   "source": [
    "# Train the model on a subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1572873967823,
     "user": {
      "displayName": "Antonia Lovjer",
      "photoUrl": "",
      "userId": "16395681258946800452"
     },
     "user_tz": 300
    },
    "id": "Tc1QhN3U-V_h",
    "outputId": "ef85b0b6-ab47-4464-c36e-a33300b7e3b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of trainset: 2000, length of validation set: 1000 , length of test set: 1000\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "from random import shuffle\n",
    "\n",
    "# training set subsample\n",
    "trainset_sub = sample(trainset, 2000)\n",
    "len(trainset_sub)\n",
    "\n",
    "# validation subample\n",
    "shuffle(valset)\n",
    "valset_sub = valset[:1000]\n",
    "\n",
    "# subsample the training set from the validation for now!\n",
    "testset_sub = valset[1000:2000]\n",
    "print('length of trainset: ' + str(len(trainset_sub)) + ', length of validation set: ' +  str(len(valset_sub)), \n",
    "      ', length of test set: ' + str(len(testset_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTHifZ53K8hk"
   },
   "outputs": [],
   "source": [
    "def pad_dimesions(spec):\n",
    "  '''\n",
    "    Data comes in several dimensions. Pad with zeros to get dimensions (112,1)\n",
    "  '''\n",
    "  x_offset = 1  \n",
    "  y_offset = 0\n",
    "  result = np.zeros([112, 1024])\n",
    "  result[x_offset:spec.shape[0] + x_offset, y_offset:spec.shape[1] + y_offset] = spec\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roDTcA97jzUK"
   },
   "source": [
    "Extract the audio files, read the wave files and separate the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_dl = trainset\n",
    "valset_dl = valset\n",
    "trainset_dl = trainset\n",
    "valset_dl = valset\n",
    "#test_dl = testset_sub\n",
    "\n",
    "# train\n",
    "train_filepaths = [i[2] for i in trainset_dl]\n",
    "train_labels = [i[0] for i in trainset_dl]\n",
    "valid_filepaths = [i[2] for i in valset_dl]\n",
    "val_labels = [i[0] for i in valset_dl]\n",
    "#test_filepaths = [i[2] for i in test_dl]\n",
    "#test_labels = [i[0] for i in valset_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KJ8KkYiqqPG"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    trainset_dl = trainset\n",
    "    valset_dl = valset\n",
    "    test_dl = testset_sub\n",
    "\n",
    "    # train\n",
    "    train_filepaths = [i[2] for i in trainset_dl]\n",
    "    train_audio_files = [wavfile.read(i)[1] for i in train_filepaths]\n",
    "    train_labels = [i[0] for i in trainset_dl]\n",
    "\n",
    "    train_spec = []\n",
    "    for i in range(len(train_audio_files)):\n",
    "      spec = pretty_spectrogram(train_audio_files[i].astype('float64'), fft_size = fft_size, step_size = step_size, log = True, thresh = spec_thresh)\n",
    "      height = spec.shape[0]\n",
    "      if height!=112:\n",
    "        spec = pad_dimesions(spec)\n",
    "      train_spec.append(spec)\n",
    "\n",
    "    # validation\n",
    "    valid_filepaths = [i[2] for i in valset_dl]\n",
    "    valid_audio_files = [wavfile.read(i)[1] for i in valid_filepaths]\n",
    "    val_labels = [i[0] for i in valset_dl]\n",
    "\n",
    "    valid_spec = []\n",
    "    for i in range(len(valid_audio_files)):\n",
    "      spec = pretty_spectrogram(valid_audio_files[i].astype('float64'), fft_size = fft_size, step_size = step_size, log = True, thresh = spec_thresh)\n",
    "      if height!=112:\n",
    "        spec = pad_dimesions(spec)\n",
    "      valid_spec.append(spec)\n",
    "\n",
    "    # test\n",
    "    test_filepaths = [i[2] for i in test_dl]\n",
    "    test_audio_files = [wavfile.read(i)[1] for i in test_filepaths]\n",
    "    test_labels = [i[0] for i in valset_dl]\n",
    "\n",
    "    test_spec = []\n",
    "    for i in range(len(test_audio_files)):\n",
    "      spec = pretty_spectrogram(test_audio_files[i].astype('float64'), fft_size = fft_size, step_size = step_size, log = True, thresh = spec_thresh)\n",
    "      if height!=112:\n",
    "        spec = pad_dimesions(spec)\n",
    "      test_spec.append(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    output_mean = 0.\n",
    "    output_std = 0.\n",
    "    n = 0\n",
    "    for X,y in loader:\n",
    "        output_mean += np.mean(X.detach().cpu().numpy())\n",
    "        output_std += np.std(X.detach().cpu().numpy())\n",
    "        n += 1\n",
    "        if n % 10 == 0:\n",
    "            print(n)\n",
    "    return output_mean/n, output_std/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCzvFAM1rW6x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "mean=-3.1259581955996425\n",
    "std=0.8961027914827521\n",
    "batch_size=32\n",
    "num_workers=8\n",
    "data_train_sub = CustomDatasetSimple_new(train_filepaths, train_labels, mean, std)\n",
    "data_valid_sub = CustomDatasetSimple_new(valid_filepaths, val_labels, mean, std)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=data_train_sub, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=data_valid_sub, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "#del(train_spec)\n",
    "#del(train_spec_np)\n",
    "#del(valid_spec)\n",
    "#del(test_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_mean_std(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KuqqLVJq1lQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BaseTrainer provides base functionality for any trainer object.\n",
    "It provides functionality to:\n",
    "    - train for one epoch\n",
    "    - validation \n",
    "    - testing\n",
    "\"\"\"\n",
    "import torch\n",
    "from torchvision import models\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch import optim\n",
    "\n",
    "class BaseTrainer:\n",
    "    \"\"\"Base Class for trainer/train.py.\"\"\"\n",
    "\n",
    "    def __init__(self, model, train_dl, valid_dl, test_dl, criterion, n_epochs, model_filename):\n",
    "        \"\"\"Initialize the BaseTrainer object.\"\"\"\n",
    "        self.model = model\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.criterion = criterion\n",
    "        self.opt_lrs = []\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        # self.attack = attack\n",
    "        self.model_filename = model_filename\n",
    "\n",
    "    def run_epoch(self, epoch, loader, optimizer=None, attack=None):\n",
    "      running_loss = 0.\n",
    "      running_corrects = 0.\n",
    "      running_model = 0.\n",
    "      n = 0.\n",
    "      counter_batch = 0\n",
    "      for X, y_true in loader:\n",
    "        # print('debug4')\n",
    "        start = time.time()\n",
    "        X = X.cuda()\n",
    "        y_true = y_true.cuda()\n",
    "        if optimizer!=None:\n",
    "          optimizer.zero_grad()\n",
    "        if attack!=None:\n",
    "          delta = attack(model, X, y_true)\n",
    "          delta = delta.cuda()\n",
    "          y_pred = self.model(X + delta)\n",
    "        else:\n",
    "          # print('debug5')\n",
    "          y_pred = self.model(X)\n",
    "        loss = self.criterion(y_pred, y_true) # input criterion is negative\n",
    "        if optimizer!=None:\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        indices = torch.argmax(y_pred, dim=1)\n",
    "        running_loss += float(loss)\n",
    "        running_corrects += np.sum(indices.detach().cpu().numpy() == y_true.detach().cpu().numpy())\n",
    "        n += y_true.detach().cpu().numpy().shape[0]\n",
    "        counter_batch += 1\n",
    "        end = time.time()\n",
    "        delta_model = end - start\n",
    "        running_model += delta_model\n",
    "        if (counter_batch%20)==0:\n",
    "            print(counter_batch)\n",
    "            log_textfile(LOGFILE_PATH, '[epoch: %d, batch:  %5d] loss: %.5f time model: %.5f acc: %.5f' % (epoch + 1, counter_batch + 1, running_loss/n, running_model / 20.0, running_corrects / 20))\n",
    "        # if optimizer==None:\n",
    "        #   print('loss: ' + str(running_loss/n) + ' accuracy: ' + str(running_corrects/n))\n",
    "      return(running_loss/n, running_corrects/n)\n",
    "    \n",
    "    def fit_model_new(self, optimizer, n_epochs, model_filename, attack):\n",
    "        \"\"\"\n",
    "        Perform one training epoch.\n",
    "\n",
    "        Parameters:\n",
    "            optimizer - optimizer to use while training\n",
    "            scheduler - scheduler to use while training\n",
    "        Returns: training loss after epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        final_loss = None\n",
    "        lowest_valid_loss = 99999999\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "          self.model.train()\n",
    "          train_loss, train_acc = self.run_epoch(epoch, self.train_dl, optimizer, attack=attack)\n",
    "          self.model.eval()\n",
    "          valid_loss, valid_acc = self.run_epoch(epoch, self.valid_dl, None, attack=attack)\n",
    "          if valid_loss < lowest_valid_loss:\n",
    "            lowest_valid_loss = valid_loss\n",
    "            torch.save(self.model, 'saved/' + model_filename)\n",
    "          log_textfile(LOGFILE_PATH, 'epoch:' + str(epoch + 1) + ' train loss: ' + str(train_loss) + ' train acc: ' + str(train_acc) + ' valid loss: ' + str(valid_loss) + ' valid acc: ' + str(valid_acc))\n",
    "          print('epoch:' + str(epoch + 1) + ' train loss: ' + str(train_loss) + ' train acc: ' + str(train_acc) + ' valid loss: ' + str(valid_loss) + ' valid acc: ' + str(valid_acc))\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AbhRW2NrujN"
   },
   "outputs": [],
   "source": [
    "MODELNAME = 'full_dataset_normal'\n",
    "LOGFILE_PATH = 'logs/' + MODELNAME\n",
    "\n",
    "from models.resnet import ResNet, resnet34\n",
    "\n",
    "model = resnet34(pretrained=False, progress=False).cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmWh9stFrvLE"
   },
   "outputs": [],
   "source": [
    "trainer = BaseTrainer(model=model, train_dl=train_data_loader, valid_dl=valid_data_loader, test_dl=valid_data_loader, criterion=criterion, model_filename=MODELNAME, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[epoch: 1, batch:     21] loss: 0.07373 time model: 0.66903 acc: 6.35000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-8a961b8b7a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODELNAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_model_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODELNAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-269-d64e9f22e64a>\u001b[0m in \u001b[0;36mfit_model_new\u001b[0;34m(self, optimizer, n_epochs, model_filename, attack)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m           \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m           \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-269-d64e9f22e64a>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, epoch, loader, optimizer, attack)\u001b[0m\n\u001b[1;32m     57\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODELNAME = 'full_dataset_normal'\n",
    "LOGFILE_PATH = 'logs/' + MODELNAME\n",
    "global_eps = 0.3\n",
    "\n",
    "def fgsm(model, X, y, epsilon=0.3):\n",
    "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    delta = delta.cuda()\n",
    "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "    loss.backward()\n",
    "    return epsilon * delta.grad.detach().sign()\n",
    "\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=20, randomize=False):\n",
    "    \"\"\" Construct PGD adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    if randomize:\n",
    "        delta = torch.rand_like(X, requires_grad=True)\n",
    "        delta.data = delta.data * 2 * epsilon - epsilon\n",
    "    else:\n",
    "        delta = torch.zeros_like(X, requires_grad=True)\n",
    "        \n",
    "    for t in range(num_iter):\n",
    "        loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "from models.resnet import ResNet, resnet34\n",
    "model = resnet34(pretrained=False, progress=False).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "trainer = BaseTrainer(model=model, train_dl=train_data_loader, valid_dl=valid_data_loader, test_dl=valid_data_loader, criterion=criterion, model_filename=MODELNAME, n_epochs=1)\n",
    "trainer.fit_model_new(optimizer=optim.Adam(model.parameters(), lr=.001), n_epochs=10, model_filename=MODELNAME, attack=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'full_dataset_fgsm03'\n",
    "LOGFILE_PATH = 'logs/' + MODELNAME\n",
    "global_eps = 0.3\n",
    "\n",
    "def fgsm(model, X, y, epsilon=0.3):\n",
    "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    delta = delta.cuda()\n",
    "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "    loss.backward()\n",
    "    return epsilon * delta.grad.detach().sign()\n",
    "\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=20, randomize=False):\n",
    "    \"\"\" Construct PGD adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    if randomize:\n",
    "        delta = torch.rand_like(X, requires_grad=True)\n",
    "        delta.data = delta.data * 2 * epsilon - epsilon\n",
    "    else:\n",
    "        delta = torch.zeros_like(X, requires_grad=True)\n",
    "        \n",
    "    for t in range(num_iter):\n",
    "        loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "from models.resnet import ResNet, resnet34\n",
    "model = resnet34(pretrained=False, progress=False).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "trainer = BaseTrainer(model=model, train_dl=train_data_loader, valid_dl=valid_data_loader, test_dl=valid_data_loader, criterion=criterion, model_filename=MODELNAME, n_epochs=1)\n",
    "trainer.fit_model_new(optimizer=optim.Adam(model.parameters(), lr=.001), n_epochs=10, model_filename=MODELNAME, attack=fgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'full_dataset_fgsm20'\n",
    "LOGFILE_PATH = 'logs/' + MODELNAME\n",
    "global_eps = 2.\n",
    "\n",
    "def fgsm(model, X, y, epsilon=0.3):\n",
    "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    delta = delta.cuda()\n",
    "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "    loss.backward()\n",
    "    return epsilon * delta.grad.detach().sign()\n",
    "\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=20, randomize=False):\n",
    "    \"\"\" Construct PGD adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    if randomize:\n",
    "        delta = torch.rand_like(X, requires_grad=True)\n",
    "        delta.data = delta.data * 2 * epsilon - epsilon\n",
    "    else:\n",
    "        delta = torch.zeros_like(X, requires_grad=True)\n",
    "        \n",
    "    for t in range(num_iter):\n",
    "        loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "from models.resnet import ResNet, resnet34\n",
    "model = resnet34(pretrained=False, progress=False).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "trainer = BaseTrainer(model=model, train_dl=train_data_loader, valid_dl=valid_data_loader, test_dl=valid_data_loader, criterion=criterion, model_filename=MODELNAME, n_epochs=1)\n",
    "trainer.fit_model_new(optimizer=optim.Adam(model.parameters(), lr=.001), n_epochs=10, model_filename=MODELNAME, attack=fgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'full_dataset_pdg03'\n",
    "LOGFILE_PATH = 'logs/' + MODELNAME\n",
    "global_eps = 0.3\n",
    "\n",
    "def fgsm(model, X, y, epsilon=0.3):\n",
    "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    delta = delta.cuda()\n",
    "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "    loss.backward()\n",
    "    return epsilon * delta.grad.detach().sign()\n",
    "\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=7, randomize=False):\n",
    "    \"\"\" Construct PGD adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    if randomize:\n",
    "        delta = torch.rand_like(X, requires_grad=True)\n",
    "        delta.data = delta.data * 2 * epsilon - epsilon\n",
    "    else:\n",
    "        delta = torch.zeros_like(X, requires_grad=True)\n",
    "        \n",
    "    for t in range(num_iter):\n",
    "        loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "from models.resnet import ResNet, resnet34\n",
    "model = resnet34(pretrained=False, progress=False).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "trainer = BaseTrainer(model=model, train_dl=train_data_loader, valid_dl=valid_data_loader, test_dl=valid_data_loader, criterion=criterion, model_filename=MODELNAME, n_epochs=1)\n",
    "trainer.fit_model_new(optimizer=optim.Adam(model.parameters(), lr=.001), n_epochs=10, model_filename=MODELNAME, attack=pgd_linf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'full_dataset_pdg20'\n",
    "LOGFILE_PATH = 'logs/' + MODELNAME\n",
    "global_eps = 2.0\n",
    "\n",
    "def fgsm(model, X, y, epsilon=0.3):\n",
    "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    delta = delta.cuda()\n",
    "    loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "    loss.backward()\n",
    "    return epsilon * delta.grad.detach().sign()\n",
    "\n",
    "\n",
    "def pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=7, randomize=False):\n",
    "    \"\"\" Construct PGD adversarial examples on the examples X\"\"\"\n",
    "    epsilon = global_eps\n",
    "    if randomize:\n",
    "        delta = torch.rand_like(X, requires_grad=True)\n",
    "        delta.data = delta.data * 2 * epsilon - epsilon\n",
    "    else:\n",
    "        delta = torch.zeros_like(X, requires_grad=True)\n",
    "        \n",
    "    for t in range(num_iter):\n",
    "        loss = nn.CrossEntropyLoss()(model(X + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "from models.resnet import ResNet, resnet34\n",
    "model = resnet34(pretrained=False, progress=False).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "trainer = BaseTrainer(model=model, train_dl=train_data_loader, valid_dl=valid_data_loader, test_dl=valid_data_loader, criterion=criterion, model_filename=MODELNAME, n_epochs=1)\n",
    "trainer.fit_model_new(optimizer=optim.Adam(model.parameters(), lr=.001), n_epochs=10, model_filename=MODELNAME, attack=pgd_linf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def load_model(name):\n",
    "    model = torch.load(name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalTrain1 = load_model('saved/normalTrain1')\n",
    "full_dataset_fgsm03 = load_model('saved/full_dataset_fgsm03')\n",
    "full_dataset_fgsm20 = load_model('saved/full_dataset_fgsm20')\n",
    "full_dataset_pdg03 = load_model('saved/full_dataset_pdg03')\n",
    "full_dataset_normal = load_model('saved/full_dataset_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "LOGFILE_PATH = 'logs/eval/normalTrain1'\n",
    "trainer_normalTrain1 = BaseTrainer(model=normalTrain1, \n",
    "                                   train_dl=train_data_loader, \n",
    "                                   valid_dl=valid_data_loader, \n",
    "                                   test_dl=valid_data_loader, \n",
    "                                   criterion=criterion, \n",
    "                                   model_filename='eval_normalTrain1', \n",
    "                                   n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 11.17 GiB total capacity; 9.95 GiB already allocated; 10.94 MiB free; 205.02 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6f191d345210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer_normalTrain1 = trainer_normalTrain1.run_epoch(1, valid_data_loader, \n\u001b[1;32m      2\u001b[0m                                                       \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                                       attack=None)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-d64e9f22e64a>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, epoch, loader, optimizer, attack)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0;31m# print('debug5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input criterion is negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/models/resnet.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1655\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m     )\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 11.17 GiB total capacity; 9.95 GiB already allocated; 10.94 MiB free; 205.02 MiB cached)"
     ]
    }
   ],
   "source": [
    "trainer_normalTrain1 = trainer_normalTrain1.run_epoch(1, valid_data_loader, \n",
    "                                                      optimizer=None, \n",
    "                                                      attack=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(np.moveaxis(dd[0], 0, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Data/train/audio/on/6a700f9d_nohash_1.wav'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_sub[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, data = wavfile.read(trainset_sub[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = np.squeeze(np.moveaxis(dd[10], 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 1024)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(10*np.random.normal(size=output1.shape), a_max=10, a_min=-10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddd = np.random.normal(size=(112,1024))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = np.clip(dddd, a_max=3, a_min=-3)*std+mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(ddd-dddd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (output1*std+mean)+ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered_audio_orig = invert_pretty_spectrogram(output, fft_size = fft_size,\n",
    "                                            step_size = step_size, log = True, n_iter = 10)\n",
    "IPython.display.Audio(data=recovered_audio_orig, rate=rate) # play the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAXvblzEBTTq"
   },
   "outputs": [],
   "source": [
    "# normal training, fgsm attack\n",
    "trainer.run_epoch(valid_data_loader, optimizer=None, attack=fgsm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "audio_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
